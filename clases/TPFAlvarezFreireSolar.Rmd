---
title: "Trabajo práctico final"
author: "Alvarez Nicolás, Freire Guido, Solar Facundo"
date: "2023-12-08"
output: html_document
---

```{r}
library(tidyverse)
library(readxl)
```

## Lectura de datos

### (a)

Leemos los datos y renombramos las columnas. Analizamos las primeras filas.

```{r}
df <- read_excel('body.xls', col_names = FALSE)
colnames(df)<-(c("BIAC","BIIL","BITRO","CHEST1","CHEST2","ELBOW","WRIST", "KNEE","ANKLE","SHOUL","CHESTG","WAISTG","NAVEL","HIP","GLUTE","BICEP", "FLOREA","KNEEG","CALF","ANKLEG","WRISTG","AGE","WEIG","HEIG","GEN"))
head(df)
```

```{r}
colSums(is.na(df))
```

No hay NAs en ninguna columna de `df` .

## Etapa exploratoria

### (b)

Agarramos los WEIG según GEN y guardamos en `weig_0` y `weig_1`, exploramos.

```{r}
df$GEN <- as.factor(df$GEN)
df %>% group_by(GEN) -> df_GEN
```

Boxploteamos WEIG según GEN.

```{r}
df %>% ggplot(aes(GEN, WEIG)) + geom_boxplot()
```


Implementamos un bootstrap no-paramétrico

```{r}
bootstrap = function(n, x, estad = median, s = FALSE) {
  if (!s) {
    s = length(x)
  }
  return(replicate(n, estad(sample(x, s, replace = TRUE))))
}
```

Bootstrapeamos y graficamos los intervalos:

```{r}
B = 5000
df_GEN %>%
  # Usamos (length(WEIG) - 1) * sd / length(WEIG) para usar el sesgado
  summarise(WEIGHT = median(WEIG), s = 1.96*sqrt((B-1)/B)*sd(bootstrap(B, WEIG, median))) %>%
  ggplot(aes(GEN, WEIGHT, s, ymin = WEIGHT-s, ymax = WEIGHT+s)) + geom_linerange() + geom_point(aes(fill = "mediana")) + labs(title = "Intervalos de confianza con bootstrap uniforme", subtitle = "nivel = 0.95") + scale_fill_discrete(name = "")
```


```{r}
df_GEN %>% 
  summarise(WEIGHT = median(WEIG), a = quantile(bootstrap(B, WEIG, median), 0.025), b = quantile(bootstrap(B, WEIG, median), 0.975)) %>%
  ggplot(aes(GEN, WEIGHT, ymin = a, ymax = b)) + geom_linerange() + geom_point(aes(fill = "mediana")) + labs(title = "Intervalos de confianza bootstrapeados con percentil", subtitle = "nivel = 0.95") + scale_fill_discrete(name = "")
```

### (c)

```{r}
df_GEN %>% ggplot(aes(WEIG, HEIG)) + geom_point(aes(color = GEN))
```

Este gráfico nos sugiere que existe una relación positiva entre WEIG y HEIG, es decir, a mayor WEIG, mayor HEIG en general, con algunas excepciones.

### (d)

Ajustamos la regresión no paramétrica para cada GEN usando ksmooth.

```{r}
df_GEN %>% do(ksmooth(x = .$WEIG, y = .$HEIG, bandwidth = 10, kernel = "normal") %>% as.data.frame()) -> regnopar
head(regnopar)
```

```{r}
df_GEN %>%
  ggplot(aes(WEIG, HEIG)) + geom_point(aes(color = GEN), alpha = 0.1) + geom_line(data = regnopar, aes(x, y, color = GEN)) + labs(title = "Regresión Nadaraya-Watson", subtitle = "bandwidth = 10")
```

A partir de los graficos, sospechamos que la relacion es no lineal pero creciente en el caso del genero 1 (masculino), mientras que para el genero 0 la curva obtenida no es monotona, siendo creciente en algunos intervalos y decreciente en otros. Esto posiblemente se deba a que GEN 0 parece tener la mayor cantidad de datos con valores relativamente bajos de WEIG y HEIG, mientras que los de GEN 1 estan mas dispersos.

### (e)

Adaptamos la función del TP1, de paso la hacemos compatible con tidyr

```{r}
bw.loocv = function(df, x, y, grilla.h = NA, K = dnorm) {
  x <- df %>% pull({{x}})
  y <- df %>% pull({{y}})
  n <- length(x)
  if (any(is.na(grilla.h))) {
    silverman <- bw.nrd0(x)
    grilla.h <- seq(silverman * 10^(-2), silverman * 10^2, length.out = 100)
  }
  f <- matrix(NA, n, length(grilla.h))
  for (i in 1:n) {
    for (h in 1:length(grilla.h)) {
      f[i, h] <- y%*%K((x[i] - x)/grilla.h[h])
      f[i, h] <- f[i,h] - y[i] * K(0)
      f[i, h] <- f[i,h] / (sum(K((x[i] - x)/grilla.h[h])) - K(0))
    }
  }
  ecm = colMeans((y - f)^2)
  return(list("h.CV" = grilla.h[which.min(ecm)], "grilla.h" = grilla.h, "ecm" = ecm))
}
```

El output es bastante idiomático.

```{r}
grilla <- seq(0, 20, by = 0.5)
df %>% filter(GEN == 0) %>% bw.loocv(WEIG, HEIG, grilla)
```

Ploteamos:

```{r}
df_GEN %>%
  do(bw.loocv(., WEIG, HEIG, grilla) %>% as.data.frame()) %>%
  ggplot(aes(grilla.h, ecm)) + geom_point(aes(color = GEN))
```

No se ve la curvatura porque tienen valores muy distintos, pero podemos hacer dos graficos distintos

```{r}
df_GEN %>%
  do(bw.loocv(., WEIG, HEIG, grilla) %>% as.data.frame()) %>%
  ggplot(aes(grilla.h, ecm)) + geom_point(aes(color = GEN)) + facet_grid(rows = vars(GEN), scales = "free_y")
```

```{r}
df %>% filter(GEN == 1) %>%
  bw.loocv(WEIG, HEIG, grilla) -> regnopar1
regnopar1 %>% as.data.frame() %>%
  ggplot(aes(grilla.h, ecm)) + geom_point(color = "cyan3") +
  geom_vline(aes(xintercept = h.CV)) +
  labs(title = "Ventana optima para GEN = 1", subtitle = "Leave one out") + scale_fill_discrete(name = "")
```

De acá rescatamos a la ventana óptima para GEN masculino: `r regnopar1$grilla.h[which.max(regnopar1$loglikes)]`

```{r}
df %>% filter(GEN == 0) %>%
  bw.loocv(WEIG, HEIG, grilla) -> regnopar0
regnopar0 %>% as.data.frame() %>%
  ggplot(aes(grilla.h, ecm)) + geom_point(color = "coral2") +
  geom_vline(aes(xintercept = h.CV)) +
  labs(title = "Ventana optima para GEN = 0", subtitle = "Leave one out") + scale_fill_discrete(name = "")
```

Lo mismo, pero para GEN femenino: `r regnopar0$grilla.h[which.max(regnopar0$loglikes)]`

### (f)

Como ksmooth no es dplyr, es muy dificil plotear un ksmooth con bandwidth distinto por GEN. Entonces, separamos el dataset en dos df por GEN.

```{r}
df %>% filter(GEN == 0) -> df_0
df %>% filter(GEN == 1) -> df_1
```

Ahora, ploteamos las regresiones no paramétricas por GEN. Cada una usa su propia ventana óptima.

```{r}
df_GEN %>%
  ggplot(aes(WEIG, HEIG)) + geom_point(aes(color = GEN), alpha = 0.1) +
  geom_line(data = ksmooth(df_0$WEIG, y = df_0$HEIG, bandwidth = regnopar0$h.CV, kernel = "normal") %>% as.data.frame(), aes(x, y), color = "coral2") +
  geom_line(data = ksmooth(df_1$WEIG, y = df_1$HEIG, bandwidth = regnopar1$h.CV, kernel = "normal") %>% as.data.frame(), aes(x, y), color = "cyan3") +
  geom_smooth(data = df_GEN, method='lm', aes(color = GEN), se = FALSE, linetype = "dashed") + 
  labs(title = "Nadaraya-Watson vs. Cuadrados mínimos", subtitle = "ventana óptima para c/GEN")
```

Al comparar ambos modelos, preferimos el modelo de Nadaraya-Watson. Nad-watson resulta más firme frente a los outliers, y también vemos que cumple con una de las hipótesis que teníamos, que era que a mayor altura mayor peso (independientemente del género de la persona).
Concluimos que Nad-watson es el mejor modelo a la hora de ajustar


## Regresión lineal
### (g)

Separamos el df en train y test.

```{r}
train <- as.vector(read.table('TrainTest')$V1)
df_train <- df[train,]
df_test <- df[!train,]

print(c('train' = nrow(df_train), 'test' = nrow(df_test)))
```

```{r}
lmtodos <- lm(WEIG ~ ., df_train)
summarytodos <- summary(lmtodos)
colnames(summarytodos$coefficients) <- c('estimate', 'sd', 't', 'p')
summarytodos
```

```{r}
df$GEN <- as.numeric(df$GEN) - 1
heatmap(cor(df))
```
Calculamos el error de predicción empírico para el conjunto de testeo.

```{r}
sum((predict(lmtodos, df_test) - df_test$WEIG)^2)
```

### (h)

```{r}
summarytodos$coefficients %>% as.data.frame() %>% ggplot(aes(1, p)) + geom_label(label = rownames(summarytodos$coefficients))
```

```{r}
summarytodos$coefficients %>% as.data.frame() %>% ggplot(aes(1, p)) + geom_label(label = rownames(summarytodos$coefficients)) + ylim(0, 0.05)
```

#### Multicolinealidad

```{r}
coefbuenos <- summarytodos$coefficients[summarytodos$coefficients[,'p'] < 0.01, 'estimate']
colbuenas <- names(coefbuenos[2:(length(coefbuenos)-1)])
colbuenas
```

```{r}
df_bueno <- df[colbuenas]
heatmap(cor(df_bueno))
```

Para verificar multicolinealidad analizamos el rango de la matriz de correlacion.

```{r}
eigbueno <- eigen(cor(df_bueno))
eigbueno
```

Los últimos 4 autovalores son cercanos a 0, lo que indica que hay multicolinealidad en los autovectores correspondientes. Por ejemplo: el autovalor `r round(eigbueno$values[length(colbuenas)], 3)` tiene un autovector con valores altos para `r colbuenas[eigbueno$vectors[,10] > 0.05]`, quiere decir que se pueden combinar linealmente estas variables sin añadirle varianza al modelo. Teniendo esto en cuenta, seleccionamos variables sin multicolinealidad.

```{r}
colbuenas <- c("CHEST1", "KNEE", "GLUTE", "CALF", "AGE", "HEIG")
df_bueno <- df[colbuenas]
heatmap(cor(df_bueno))
```

```{r}
eigen(cor(df_bueno))
```

Ajustamos un modelo lineal con nuestra selección de variables con baja colinealidad.

```{r}
lmbueno <- lm(WEIG ~ AGE + GLUTE + HEIG + CHEST1 + CALF + KNEE, df_train)
summarybueno <- summary(lmbueno)
summarybueno
```

Comparando los modelos, observamos por una parte que el R^2 disminuyó aproximadamente en 0.1, tanto el global como el ajustado, y el error estándar aumentó en 2. Por una parte, esto nos dice que nuestro modelo con las variables que elegimos parece tener menor poder explicativo. El aumento del error estándar nos dice que este modelo tiene más error intrínseco (el error de medición del modelo lineal), aunque también es probable que quede menos pegado a los datos (menos sesgado). Por último, nuestro modelo tiene todas las variables muy significativas, salvo AGE, que es significativa a nivel 0.05.

## (i)

```{r}
library(glmnet)
```

Ahora usamos regularización de LASSO para quedarnos con un subconjunto de variables. Nos interesa ver cómo difiere la selección con este método basado en optimización con la que hicimos nosotros ad hoc.

```{r}
lassofit <- glmnet(as.matrix(df_train[,colnames(df_train) != "WEIG"]), as.matrix(df_train[,"WEIG"]))
lassofit
```

```{r}
plot(lassofit, xvar = "lambda")
```

En el gráfico, podemos observar que al incrementar el valor de lambda, eventualmente todos los coeficientes terminan siendo 0, es decir, terminan siendo penalizados. Contrastando con lo que observamos en la columna %Dev, que nos permite comparar con un modelo que tiene solo intercept, vemos que a medida que crece lambda, el modelo que tiene solo intercept va quedando atrás respecto al modelo que incorpora nuevas variables. Por lo que deberíamos elegir un lambda adecuado que nos permita explicar el fenómeno, pero a la vez no penalice a todas las variables.

## (j)

```{r}
lassocv <- cv.glmnet(as.matrix(df_train[,colnames(df_train) != "WEIG"]), as.matrix(df_train[,"WEIG"]))
```

```{r}
coef(lassofit, s = lassocv$lambda.1se)
```

Error para modelo lineal de todas las variables

```{r}
sum((predict(lmtodos, df_test) - df_test$WEIG)^2)
```

Error para modelo lineal con variables ad hoc

```{r}
sum((predict(lmbueno, df_test) - df_test$WEIG)^2)
```


Error para lambda 1se

```{r}
sum((predict(lassofit, as.matrix(df_test[,colnames(df_test) != "WEIG"]), s = lassocv$lambda.1se) - df_test$WEIG)^2)
```

Error para lambda min

```{r}
sum((predict(lassofit, as.matrix(df_test[,colnames(df_test) != "WEIG"]), s = lassocv$lambda.min) - df_test$WEIG)^2)
```

Podemos ver que este modelo utiliza las variables CHEST1, CHEST2, KNEE, SHOUL, CHESTG, WAISTG, HIP, GLUTE, BICEP, FLOREA, KNEEG, CALF, HEIG. En general, los coeficientes son menores que los que obtuvimos con el modelo del punto g), además de que tenemos más variables explicativas. Comparando los errores de predicción, vemos que el modelo ajustado con LASSO con regla de un desvió estándar tiene el menor error de predicción, por lo que es el modelo que preferimos. 

## (h)

A lo largo del TP estudiamos cómo podemos explicar la altura de una persona (variable HEIG) en base a otras variables explicativas. En un primer momento, nos centramos en la variable WEIG, esto es, el peso. Para eso planteamos regresión de Nadaraya Watson y de mínimos cuadrados, y nos inclinamos por el primer modelo por captar mejor las tendencias observadas y por ser más firme frente a los outliers, esto es, que detecta que valores atípicos de peso no necesariamente se corresponden con mayor altura. 

En una segunda etapa agregamos más variables al modelo. En un primer momento agregamos todas las variables y seleccionamos "a ojo" a partir de dicho modelo, basándonos en el p-valor y en que no haya colinealidad. Posteriormente se ajustaron modelos LASSO para elegir variables explicativas, y comparamos los errores de predicción entre todos los modelos, eligiendo finalmente el modelo LASSO con regla de un desvío estándar.
