---
title: "Trabajo práctico final"
author: "Álvarez Nicolás, Freire Guido, Solar Facundo"
date: "2023-12-08"
output: html_document
---

```{r}
library(tidyverse)
library(readxl)
```

## Lectura de datos

### (a)

Leemos los datos y renombramos las columnas. Analizamos las primeras filas.

```{r}
df <- read_excel('body.xls', col_names = FALSE)
colnames(df)<-(c("BIAC","BIIL","BITRO","CHEST1","CHEST2","ELBOW","WRIST", "KNEE","ANKLE","SHOUL","CHESTG","WAISTG","NAVEL","HIP","GLUTE","BICEP", "FLOREA","KNEEG","CALF","ANKLEG","WRISTG","AGE","WEIG","HEIG","GEN"))
head(df)
```

```{r}
colSums(is.na(df))
```

No hay NAs en ninguna columna de `df` .

## Etapa exploratoria

### (b)

Agarramos los WEIG según GEN y guardamos en `weig_0` y `weig_1`, exploramos.

```{r}
df$GEN <- as.factor(df$GEN)
df %>% group_by(GEN) -> df_GEN
```

Boxploteamos WEIG según GEN.

```{r}
df %>% ggplot(aes(GEN, WEIG)) + geom_boxplot()
```


Implementamos un bootstrap no-paramétrico

```{r}
bootstrap = function(n, x, estad = median, s = FALSE) {
  if (!s) {
    s = length(x)
  }
  return(replicate(n, estad(sample(x, s, replace = TRUE))))
}
```

Bootstrapeamos y graficamos los intervalos:

```{r}
B = 5000
df_GEN %>%
  summarise(WEIGHT = median(WEIG), s = (length(WEIG)-1)*sd(bootstrap(B, WEIG, median))/length(WEIG)) %>%
  ggplot(aes(GEN, WEIGHT, s, ymin = WEIGHT-s, ymax = WEIGHT+s)) + geom_linerange() + geom_point(aes(fill = "mediana")) + labs(title = "Intervalos de confianza con bootstrap uniforme", subtitle = "nivel = 0.95") + scale_fill_discrete(name = "")
```


```{r}
df_GEN %>% 
  summarise(WEIGHT = median(WEIG), a = quantile(bootstrap(B, WEIG, median), 0.025), b = quantile(bootstrap(B, WEIG, median), 0.975)) %>%
  ggplot(aes(GEN, WEIGHT, ymin = a, ymax = b)) + geom_linerange() + geom_point(aes(fill = "mediana")) + labs(title = "Intervalos de confianza bootstrapeados con percentil", subtitle = "nivel = 0.95") + scale_fill_discrete(name = "")
```

### (c)

```{r}
df_GEN %>% ggplot(aes(WEIG, HEIG)) + geom_point(aes(color = GEN))
```

### (d)

Ajustamos la regresión no paramétrica para cada GEN usando ksmooth.

```{r}
df_GEN %>% do(ksmooth(x = .$WEIG, y = .$HEIG, bandwidth = 10, kernel = "normal") %>% as.data.frame()) -> regnopar
head(regnopar)
```

```{r}
df_GEN %>%
  ggplot(aes(WEIG, HEIG)) + geom_point(aes(color = GEN), alpha = 0.1) + geom_line(data = regnopar, aes(x, y, color = GEN)) + labs(title = "Regresión Nadaraya-Watson", subtitle = "bandwidth = 10")
```

### (e)

Adaptamos la función del TP1, de paso la hacemos compatible con tidyr

```{r}
bw.loocv = function(df, x, y, grilla.h = NA, K = dnorm) {
  x <- df %>% pull({{x}})
  y <- df %>% pull({{y}})
  n <- length(x)
  if (any(is.na(grilla.h))) {
    silverman <- bw.nrd0(x)
    grilla.h <- seq(silverman * 10^(-2), silverman * 10^2, length.out = 100)
  }
  f <- matrix(NA, n, length(grilla.h))
  for (i in 1:n) {
    for (h in 1:length(grilla.h)) {
      f[i, h] <- sum(y%*%K((x[i] - x)/grilla.h[h]))
      f[i, h] <- f[i,h] - y[i] * K(0)
      f[i, h] <- f[i, h] / (sum(K((x[i] - x)/grilla.h[h])) - K(0))
    }
  }
  loglikes = colMeans(log(f))
  return(list("h.CV" = grilla.h[which.max(loglikes)], "grilla.h" = grilla.h, "loglikes" = loglikes))
}
```

El output es bastante idiomático.

```{r}
grilla <- seq(0, 20, by = 0.5)
df %>% filter(GEN == 0) %>% bw.loocv(WEIG, HEIG, grilla)
```

Ploteamos:

```{r}
df_GEN %>%
  do(bw.loocv(., WEIG, HEIG, grilla) %>% as.data.frame()) %>%
  ggplot(aes(grilla.h, loglikes)) + geom_point(aes(color = GEN))
```

No se ve la curvatura porque tienen valores muy distintos, pero podemos hacer dos graficos distintos

```{r}
df_GEN %>%
  do(bw.loocv(., WEIG, HEIG, grilla) %>% as.data.frame()) %>%
  ggplot(aes(grilla.h, loglikes)) + geom_point(aes(color = GEN)) + facet_grid(rows = vars(GEN), scales = "free_y")
```

```{r}
df %>% filter(GEN == 1) %>%
  bw.loocv(WEIG, HEIG, grilla) -> regnopar1
regnopar1 %>% as.data.frame() %>%
  ggplot(aes(grilla.h, loglikes)) + geom_point(color = "cyan3") +
  geom_vline(aes(xintercept = grilla.h[which.max(loglikes)])) +
  labs(title = "Ventana optima para GEN = 1", subtitle = "Leave one out") + scale_fill_discrete(name = "")
```
De acá rescatamos a la ventana óptima para GEN masculino: `r regnopar1$grilla.h[which.max(regnopar1$loglikes)]`

```{r}
df %>% filter(GEN == 0) %>%
  bw.loocv(WEIG, HEIG, grilla) -> regnopar0
regnopar0 %>% as.data.frame() %>%
  ggplot(aes(grilla.h, loglikes)) + geom_point(color = "coral2") +
  geom_vline(aes(xintercept = grilla.h[which.max(loglikes)])) +
  labs(title = "Ventana optima para GEN = 0", subtitle = "Leave one out") + scale_fill_discrete(name = "")
```

Lo mismo, pero para GEN femenino: `r regnopar0$grilla.h[which.max(regnopar0$loglikes)]`

### (f)

Como ksmooth no es dplyr, es muy dificil plotear un ksmooth con bandwidth distinto por GEN. Entonces, separamos el dataset en dos df por GEN.

```{r}
df %>% filter(GEN == 0) -> df_0
df %>% filter(GEN == 1) -> df_1
```

Ahora, ploteamos las regresiones no paramétricas por GEN. Cada una usa su propia ventana óptima.

```{r}
df_GEN %>%
  ggplot(aes(WEIG, HEIG)) + geom_point(aes(color = GEN), alpha = 0.1) +
  geom_line(data = ksmooth(df_0$WEIG, y = df_0$HEIG, bandwidth = regnopar0$h.CV, kernel = "normal") %>% as.data.frame(), aes(x, y), color = "coral2") +
  geom_line(data = ksmooth(df_1$WEIG, y = df_1$HEIG, bandwidth = regnopar1$h.CV, kernel = "normal") %>% as.data.frame(), aes(x, y), color = "cyan3") +
  geom_smooth(data = df_GEN, method='lm', aes(color = GEN), se = FALSE, linetype = "dashed") + 
  labs(title = "Nadaraya-Watson vs. Cuadrados mínimos", subtitle = "ventana óptima para c/GEN")
```

## Regresión lineal
### (g)

Separamos el df en train y test.

```{r}
train <- as.vector(read.table('TrainTest')$V1)
df_train <- df[train,]
df_test <- df[!train,]

print(c('train' = nrow(df_train), 'test' = nrow(df_test)))
```

```{r}
lmtodos <- lm(WEIG ~ ., df_train)
summarytodos <- summary(lmtodos)
colnames(summarytodos$coefficients) <- c('estimate', 'sd', 't', 'p')
summarytodos
```

```{r}
df$GEN <- as.numeric(df$GEN) - 1
heatmap(cor(df))
```
Calculamos el error de predicción empírico para el conjunto de testeo.

```{r}
sum((predict(lmtodos, df_test) - df_test$WEIG)^2)
```

### (h)

```{r}
summarytodos$coefficients %>% as.data.frame() %>% ggplot(aes(1, p)) + geom_label(label = rownames(summarytodos$coefficients))
```

```{r}
summarytodos$coefficients %>% as.data.frame() %>% ggplot(aes(1, p)) + geom_label(label = rownames(summarytodos$coefficients)) + ylim(0, 0.05)
```

#### Multicolinealidad

```{r}
coefbuenos <- summarytodos$coefficients[summarytodos$coefficients[,'p'] < 0.01, 'estimate']
colbuenas <- names(coefbuenos[2:(length(coefbuenos)-1)])
colbuenas
```

```{r}
df_bueno <- df[colbuenas]
heatmap(cor(df_bueno))
```

Para verificar multicolinealidad analizamos el rango de la matriz de correlacion.

```{r}
eigbueno <- eigen(cor(df_bueno))
eigbueno
```

Los últimos 4 autovalores son cercanos a 0, lo que indica que hay multicolinealidad en los autovectores correspondientes. Por ejemplo: el autovalor `r round(eigbueno$values[length(colbuenas)], 3)` tiene un autovector con valores altos para `r colbuenas[eigbueno$vectors[,10] > 0.05]`, quiere decir que se pueden combinar linealmente estas variables sin añadirle varianza al modelo. Teniendo esto en cuenta, seleccionamos variables sin multicolinealidad.

```{r}
colbuenas <- c("CHEST1", "KNEE", "GLUTE", "CALF", "AGE", "HEIG")
df_bueno <- df[colbuenas]
heatmap(cor(df_bueno))
```

```{r}
eigen(cor(df_bueno))
```

Ajustamos un modelo lineal con nuestra selección de variables con baja colinealidad.

```{r}
lmbueno <- lm(WEIG ~ AGE + GLUTE + HEIG + CHEST1 + CALF + KNEE, df_train)
summarybueno <- summary(lmbueno)
summarybueno
```

## (i)

```{r}
library(glmnet)
```

Ahora usamos regularización de LASSO para quedarnos con un subconjunto de variables. Nos interesa ver cómo difiere la selección con este método basado en optimización con la que hicimos nosotros ad hoc.

```{r}
lassofit <- glmnet(as.matrix(df_train[,colnames(df_train) != "WEIG"]), as.matrix(df_train[,"WEIG"]))
lassofit
```

```{r}
plot(lassofit, xvar = "lambda")
```

## (j)

```{r}
lassocv <- cv.glmnet(as.matrix(df_train[,colnames(df_train) != "WEIG"]), as.matrix(df_train[,"WEIG"]))
```

```{r}
coef(lassofit, s = lassocv$lambda.1se)
```

Error para modelo lineal de todas las variables

```{r}
sum((predict(lmtodos, df_test) - df_test$WEIG)^2)
```

Error para modelo lineal con variables ad hoc

```{r}
sum((predict(lmbueno, df_test) - df_test$WEIG)^2)
```


Error para lambda 1se

```{r}
sum((predict(lassofit, as.matrix(df_test[,colnames(df_test) != "WEIG"]), s = lassocv$lambda.1se) - df_test$WEIG)^2)
```

Error para lambda min

```{r}
sum((predict(lassofit, as.matrix(df_test[,colnames(df_test) != "WEIG"]), s = lassocv$lambda.min) - df_test$WEIG)^2)
```

